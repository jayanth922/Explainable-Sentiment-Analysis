{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers gradio matplotlib torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL6306d849ut",
        "outputId": "5b0c87aa-0d30-4bf1-85c7-f251223a9e89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "ds = tfds.load(\"imdb_reviews\", split=\"test\", as_supervised=True)\n",
        "\n",
        "\n",
        "data = []\n",
        "for text, label in ds:\n",
        "    lbl = int(label.numpy())\n",
        "    txt = text.numpy().decode(\"utf-8\")\n",
        "    data.append({\"text\": txt, \"label\": lbl})\n",
        "    if len([d for d in data if d[\"label\"]==1]) >= 100 and len([d for d in data if d[\"label\"]==0]) >= 100:\n",
        "        break\n",
        "\n",
        "\n",
        "pos = [d for d in data if d[\"label\"]==1][:100]\n",
        "neg = [d for d in data if d[\"label\"]==0][:100]\n",
        "samples = pos + neg\n",
        "random.shuffle(samples)"
      ],
      "metadata": {
        "id": "TMeUcYFH4_Wv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cot_tpl = (\n",
        "    \"Review:\\n{text}\\n\"\n",
        "    \"Let's think step by step whether this is Positive or Negative.\\nAnswer:\"\n",
        ")\n",
        "\n",
        "\n",
        "examples = samples[:3]\n",
        "few_shot_header = \"\\n\\n\".join(\n",
        "    f\"Review:\\n{ex['text']}\\nLabel: {'Positive' if ex['label']==1 else 'Negative'}\"\n",
        "    for ex in examples\n",
        ")\n",
        "\n",
        "templates = {\n",
        "    \"Direct\":       \"Review:\\n{text}\\nSentiment?\",\n",
        "    \"Few-Shot\":     few_shot_header + \"\\n\\nReview:\\n{text}\\nLabel:\",\n",
        "    \"Chain-of-Thought\": cot_tpl\n",
        "}"
      ],
      "metadata": {
        "id": "CTP0vols6vLk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3oKIfHp6v_L",
        "outputId": "57bc0d21-d51c-4f30-a45f-4201c74237c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label(prompt: str):\n",
        "    out = classifier(prompt)[0]\n",
        "    stars = int(out[\"label\"].split()[0])\n",
        "    return (\"Positive\" if stars >= 4 else \"Negative\"), out\n",
        "\n",
        "def run_style(style: str, example):\n",
        "    prompt = templates[style].format(text=example[\"text\"])\n",
        "    pred, raw = predict_label(prompt)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "2kxJtR4p6zZO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "results = {}\n",
        "for style in templates:\n",
        "    preds = [run_style(style, ex) for ex in samples]\n",
        "    truths = [\"Positive\" if ex[\"label\"]==1 else \"Negative\" for ex in samples]\n",
        "    results[style] = accuracy_score(truths, preds)\n",
        "\n",
        "# Display\n",
        "for style, acc in results.items():\n",
        "    print(f\"{style:20s}: {acc*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tm2NJXm610E",
        "outputId": "1dfdf132-32fe-409c-d2f1-ab1d64467b37"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direct              : 70.0%\n",
            "Few-Shot            : 50.0%\n",
            "Chain-of-Thought    : 73.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "results = {}\n",
        "for style in templates:\n",
        "    preds = [run_style(style, ex) for ex in samples]\n",
        "    truths = [\"Positive\" if ex[\"label\"]==1 else \"Negative\" for ex in samples]\n",
        "    results[style] = accuracy_score(truths, preds)\n",
        "\n",
        "# Display\n",
        "for style, acc in results.items():\n",
        "    print(f\"{style:20s}: {acc*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO0j6-Ri6_zB",
        "outputId": "c5c176aa-dc68-4d15-868e-7318cb1b7039"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direct              : 70.0%\n",
            "Few-Shot            : 50.0%\n",
            "Chain-of-Thought    : 73.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE_G2THvAuqu",
        "outputId": "488dcb57-8ab2-4fb9-c9e6-07d2c695ea57"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, output_attentions=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "KfOWXffSBuT1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def forward_fn(input_ids, attention_mask=None):\n",
        "    return model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "ig = IntegratedGradients(forward_fn)\n",
        "\n",
        "\n",
        "def make_prob_figure(logits):\n",
        "    probs  = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    labels = [f\"{i+1}★\" for i in range(len(probs))]\n",
        "    fig, ax = plt.subplots(figsize=(4,2.5))\n",
        "    ax.bar(labels, probs)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.set_title(\"Confidence per Rating\")\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "RYQPR5OABuzG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def make_attn_heatmap(attns, inputs):\n",
        "    attn   = attns[0][0][0].cpu().numpy()  # layer1/head1\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    im = ax.imshow(attn, aspect='auto')\n",
        "    ax.set_xticks(range(len(tokens)))\n",
        "    ax.set_xticklabels(tokens, rotation=90, fontsize=6)\n",
        "    ax.set_yticks(range(len(tokens)))\n",
        "    ax.set_yticklabels(tokens, fontsize=6)\n",
        "    ax.set_title(\"Layer 1 Head 1 Attention\")\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "a9tibIx4Bvq5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def explain_with_ig(text):\n",
        "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "    mask      = enc.get(\"attention_mask\")\n",
        "\n",
        "    logits    = model(**enc).logits\n",
        "    pred_idx  = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    atts, _   = ig.attribute(\n",
        "        inputs=input_ids,\n",
        "        additional_forward_args=(mask,),\n",
        "        target=pred_idx,\n",
        "        return_convergence_delta=False\n",
        "    )\n",
        "    scores    = atts.sum(dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    norm      = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
        "    tokens    = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "    spans     = []\n",
        "    for tok, sc in zip(tokens, norm):\n",
        "        color = f\"rgba(255,0,0,{sc:.2f})\"\n",
        "        spans.append(f\"<span style='background:{color}'>{html.escape(tok)}</span>\")\n",
        "    return \" \".join(spans)\n"
      ],
      "metadata": {
        "id": "j8OpHgLXBwfM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "\n",
        "def forward_emb(inputs_embeds, attention_mask):\n",
        "\n",
        "    outputs = model(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    return outputs.logits\n",
        "\n",
        "\n",
        "ig = IntegratedGradients(forward_emb)\n",
        "\n",
        "def explain_with_ig(text):\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "    input_ids      = enc[\"input_ids\"]\n",
        "    attention_mask = enc[\"attention_mask\"]\n",
        "\n",
        "\n",
        "    embed_layer   = model.get_input_embeddings()\n",
        "    embeddings    = embed_layer(input_ids)\n",
        "    baseline_embs = torch.zeros_like(embeddings)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs_embeds=embeddings, attention_mask=attention_mask).logits\n",
        "    pred_idx = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "\n",
        "    attributions, delta = ig.attribute(\n",
        "        inputs=embeddings,\n",
        "        baselines=baseline_embs,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=pred_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "\n",
        "    scores = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "    norm   = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "    spans  = []\n",
        "    for tok, sc in zip(tokens, norm):\n",
        "        color = f\"rgba(255,0,0,{sc:.2f})\"\n",
        "        spans.append(f\"<span style='background:{color}'>{tok}</span>\")\n",
        "\n",
        "    return \" \".join(spans)"
      ],
      "metadata": {
        "id": "3jamRZJmBxI0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gr_examples = [\n",
        "    [\"I absolutely loved this movie—brilliant acting and a great story!\", \"Direct\"],\n",
        "    [\"What a waste of time. The plot was dull and the characters were uninteresting.\", \"Direct\"],\n",
        "    [\"I wouldn’t say it was bad.\", \"Few-Shot\"],\n",
        "    [\"The cinematography was breathtaking, but the story felt painfully predictable.\", \"Chain-of-Thought\"],\n",
        "    [\"Oh great, another “groundbreaking” sequel—just what we needed.\", \"Chain-of-Thought\"],\n",
        "    [\"This movie is so atrocious, it became my guilty-pleasure highlight of the year.\", \"Few-Shot\"],\n",
        "    [\"I laughed, I cried, and I frankly questioned my life choices afterward.\", \"Chain-of-Thought\"],\n",
        "]"
      ],
      "metadata": {
        "id": "osi7c9a1B8yc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=explain_predict_with_ig,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=5, label=\"Movie Review\"),\n",
        "        gr.Dropdown(choices=list(templates.keys()), label=\"Prompt Style\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Predicted Label\"),\n",
        "        gr.JSON(label=\"Raw Model Output\"),\n",
        "        gr.Plot(label=\"Probability Distribution\"),\n",
        "        gr.Plot(label=\"Attention Heatmap\"),\n",
        "        gr.HTML(label=\"Token Attributions\"),\n",
        "    ],\n",
        "    title=\"Explainable Sentiment Analysis Playground\",\n",
        "    description=\"Label + Confidence + Attention + Integrated Gradients\",\n",
        "    examples=gr_examples,\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "XiifgyaNBx2R",
        "outputId": "d578c3da-09b2-4a5a-abb0-46de30a420d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b88a6f792716bde205.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b88a6f792716bde205.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}